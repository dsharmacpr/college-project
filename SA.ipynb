{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HeY0hvLOOh-Z"
   },
   "source": [
    "Taking the positive and negative reviews of 2 politician: Politican X and Politician Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RhcDJo7IBh8q",
    "outputId": "f555aac4-0812-415b-884a-bbe2a7475df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-NA3hedYOt7_"
   },
   "source": [
    "Working for politician X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPoNFTp2gXaf"
   },
   "outputs": [],
   "source": [
    "with open('drive/My Drive/PoliticalLeaderX.txt', 'r') as f2:\n",
    "    data = f2.read()\n",
    "    #print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "6-ZB4-ybDjjW",
    "outputId": "826638e9-5349-4593-e707-3607c0896ae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rMWyUPc-Dm1j"
   },
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tjTK-45bFqsV"
   },
   "outputs": [],
   "source": [
    "from  textblob import TextBlob\n",
    "blob = TextBlob(data.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MnfrfncKGeCH"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "header_name = ['labels', 'data']\n",
    "with open('sentiment.csv', 'w') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=header_name)\n",
    "    writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IHEDomPHDpEF"
   },
   "outputs": [],
   "source": [
    "for sent in blob.sentences:\n",
    "  #print(sent.sentiment.polarity)\n",
    "  with open('sentiment.csv', 'a') as file:\n",
    "    pol=sent.sentiment.polarity\n",
    "    if(pol<0):\n",
    "      pol=-1\n",
    "    else:\n",
    "      pol=1\n",
    "    writer = csv.DictWriter(file, fieldnames=header_name)\n",
    "    info = {\n",
    "                'labels': pol,\n",
    "                'data': sent\n",
    "            }\n",
    "    writer.writerow(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5CcI7cebE1mb"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "tkTX5Gbsdm-3",
    "outputId": "685dafdf-7bef-4fd2-ff5d-48078f46f00d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JVD9H-CYdqVG"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BXLP1n8Gii4J"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OT3f3ydji49S",
    "outputId": "57e2381e-246e-40d1-c9ee-8cbc50e423f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['data'][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "TqaFwEHfDIhW",
    "outputId": "5de8e9d8-acd9-4e70-dfdd-c19f57d8cb38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        labels                                               data\n",
       "0           1  india s shock gdp growth rate is a crisis modi...\n",
       "1          -1  most analysts had   belatedly   forecast the b...\n",
       "2           1  it is now clear that if the government does no...\n",
       "3           1  the economy is on a cusp from where it can swi...\n",
       "4           1  nirmala sitharaman is on test.if we get past t...\n",
       "...       ...                                                ...\n",
       "14356      -1      because the advertising market is broken too.\n",
       "14357       1  if you think we deserve your support  do join ...\n",
       "14358       1  your support will define our journalism  and t...\n",
       "14359      -1      it will take just a few seconds of your time.\n",
       "14360       1            support our journalismshow full article\n",
       "\n",
       "[14361 rows x 2 columns]>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rMEFURO6gT3O"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "allowed_word_types = [\"J\"]\n",
    "all_words=[]\n",
    "for p in  df['data']:\n",
    "    cleaned = re.sub(r'[^(a-zA-Z)\\s]','', p)\n",
    "    \n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    \n",
    "    stopped = [w for w in tokenized if not w in stopwords.words('english')]\n",
    "    \n",
    "    pos = nltk.pos_tag(stopped)\n",
    "    \n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwGizdOgt2wE"
   },
   "source": [
    "Bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUjFyWNLkDo1"
   },
   "outputs": [],
   "source": [
    "# creating a frequency distribution of each adjectives.\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "# listing the 1000 most frequent words\n",
    "word_features = list(all_words.keys())[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBEskg9ujo7N"
   },
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9vpXVwk2y7Cd",
    "outputId": "17d086f6-31df-4fd7-8080-8a3582b7b443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14361\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "dataset=[]\n",
    "i=0\n",
    "for p in df['data']:\n",
    "  feature=find_features(p)\n",
    "  dataset.append([feature,df['labels'][i]])\n",
    "  i+=1\n",
    "random.shuffle(dataset)\n",
    "\n",
    "print(len(dataset))# dataset is a bag of model of vectors\n",
    "\n",
    "training_set = dataset[:10000]\n",
    "testing_set = dataset[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7r51Tz976jgw"
   },
   "source": [
    "Machine Learning Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D4OOb3HMLE58",
    "outputId": "d0213bf2-9b60-4b1a-d821-ec4554721681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 86.44806237101582\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "8Q3W6qfMQHDG",
    "outputId": "d06190b6-48a8-4e35-d074-7c66cdb8f69c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False))>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# training various models by passing in the sklearn models into the SklearnClassifier from NLTK \n",
    "\n",
    "MNB_clf = SklearnClassifier(MultinomialNB())\n",
    "MNB_clf.train(training_set)\n",
    "\n",
    "BNB_clf = SklearnClassifier(BernoulliNB())\n",
    "BNB_clf.train(training_set)\n",
    "\n",
    "LogReg_clf = SklearnClassifier(LogisticRegression())\n",
    "LogReg_clf.train(training_set)\n",
    "\n",
    "SVC_clf = SklearnClassifier(SVC())\n",
    "SVC_clf.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "7v1QviP1QHl7",
    "outputId": "bf914a2c-eefa-4bc5-adbe-2a068ed7ebf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SklearnClassifier(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))>\n",
      "Multinomial Naive Baeyes classifier accuracy : 0.8651685393258427\n",
      "Multinomial Naive Baeyes f1 score : 0.9203899268887085\n",
      "Bernoulli Naive Baeyes classifier accuracy : 0.8601238248108232\n",
      "Bernoulli Naive Baeyes f1 score : 0.9170068027210885\n",
      "Logistic Regression classifier accuracy : 0.8791561568447603\n",
      "Logistic Regression f1 score : 0.929479459387127\n",
      "Support Vector classifier accuracy : 0.8793854620499886\n",
      "Support Vector f1 score : 0.9306800210859252\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "ground_truth = [r[1] for r in testing_set]\n",
    "predictions = []\n",
    "f1_scores = {}\n",
    "Li={0:MNB_clf,1:BNB_clf,2:LogReg_clf,3:SVC_clf}\n",
    "print(Li[0])\n",
    "for x in Li:\n",
    "  predictions=[]\n",
    "  for r in testing_set:\n",
    "    predictions.append(Li[x].classify(r[0]))\n",
    "  if(x==0):\n",
    "    str='Multinomial Naive Baeyes'\n",
    "  elif(x==1):\n",
    "    str='Bernoulli Naive Baeyes'\n",
    "  elif(x==2):\n",
    "    str='Logistic Regression'\n",
    "  else:\n",
    "    str='Support Vector'\n",
    "  print(str,'classifier accuracy :',accuracy_score(ground_truth,predictions))\n",
    "  print(str,'f1 score :',f1_score(ground_truth,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-0tV-hcFpHj"
   },
   "source": [
    "So we clearly see that Logistic regression classifier and Support vector perfectly classifies our dataset\n",
    "But since SVM has more f1-score so we will make predictions based on SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-dPyRV2s7UwJ"
   },
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "for r in testing_set:\n",
    "  predictions.append(SVC_clf.classify(r[0]))\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OUC3fTEAYfKW"
   },
   "outputs": [],
   "source": [
    "positive=0\n",
    "negative=0\n",
    "for i in range(0,len(predictions)):\n",
    "  if predictions[i]==1:\n",
    "    positive=positive+1\n",
    "  else:\n",
    "    negative=negative+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EJpUyGp9aMiZ",
    "outputId": "9bb5aa2b-74cd-48bd-ce48-a00097e5ebd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4030 331\n"
     ]
    }
   ],
   "source": [
    "print(positive,negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2T62gdX3ajLy"
   },
   "source": [
    "Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AERo3yKAaQAG"
   },
   "outputs": [],
   "source": [
    "positive_percent=positive/(positive+negative)\n",
    "negative_percent=negative/(positive+negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Xfr7yL_9axH0",
    "outputId": "c3eb5097-46de-4adf-fc37-79a1206869de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.40999770694795 7.590002293052052\n"
     ]
    }
   ],
   "source": [
    "print(positive_percent*100,negative_percent*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RnqttI86bESR"
   },
   "source": [
    "So nearly 92% people are in favour of politician X and around 8% people do not favour him"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "urTuFq9OBbeA"
   },
   "source": [
    "Taking new Data for another politician (politician Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pQhqemxga7cr"
   },
   "outputs": [],
   "source": [
    "data=''\n",
    "with open('drive/My Drive/PoliticalLeaderY.txt', 'r') as f2:\n",
    "    data = f2.read()\n",
    "    #print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDMaTwMO0qwP"
   },
   "outputs": [],
   "source": [
    "from  textblob import TextBlob\n",
    "blob = TextBlob(data.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2R6RgpZ3-mo"
   },
   "outputs": [],
   "source": [
    "header_name = ['labels', 'data']\n",
    "with open('sentiment1.csv', 'w') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=header_name)\n",
    "    writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jeauwrd64Jk8"
   },
   "outputs": [],
   "source": [
    "for sent in blob.sentences:\n",
    "  #print(sent.sentiment.polarity)\n",
    "  with open('sentiment1.csv', 'a') as file:\n",
    "    pol=sent.sentiment.polarity\n",
    "    if(pol<0):\n",
    "      pol=-1\n",
    "    else:\n",
    "      pol=1\n",
    "    writer = csv.DictWriter(file, fieldnames=header_name)\n",
    "    info = {\n",
    "                'labels': pol,\n",
    "                'data': sent\n",
    "            }\n",
    "    writer.writerow(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LwsFN_5z4OaN"
   },
   "outputs": [],
   "source": [
    "df=[]\n",
    "import pandas as pd\n",
    "df=pd.read_csv('sentiment1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "O8bfcTe35etj",
    "outputId": "27fc2b7a-cdb2-48e1-97fd-2519b4b89992"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        labels                                               data\n",
       "0           1         no cure yet for covid  or congress crisis.\n",
       "1           1  rahul gandhi continuously fails to win india\\n...\n",
       "2           1  just as there is no single medicine to cure co...\n",
       "3           1  in the absence of any vaccine  social distanci...\n",
       "4           1  similarly  the need for a strong central leade...\n",
       "...       ...                                                ...\n",
       "19215      -1      because the advertising market is broken too.\n",
       "19216       1  if you think we deserve your support  do join ...\n",
       "19217       1  your support will define our journalism  and t...\n",
       "19218      -1      it will take just a few seconds of your time.\n",
       "19219       1            support our journalismshow full article\n",
       "\n",
       "[19220 rows x 2 columns]>"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wkj8c0b-5nvq"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "allowed_word_types = [\"J\"]\n",
    "all_words=[]\n",
    "for p in  df['data']:\n",
    "    cleaned = re.sub(r'[^(a-zA-Z)\\s]','', p)\n",
    "    \n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    \n",
    "    stopped = [w for w in tokenized if not w in stopwords.words('english')]\n",
    "    \n",
    "    pos = nltk.pos_tag(stopped)\n",
    "    \n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6iLrISylBky8"
   },
   "source": [
    "Making Bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6Aw2zZ15u9H"
   },
   "outputs": [],
   "source": [
    "# creating a frequency distribution of each adjectives.\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "# listing the 1000 most frequent words\n",
    "word_features = list(all_words.keys())[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ya53Apmr54cG"
   },
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dkFHW-5z58gx",
    "outputId": "6c18022d-0317-407c-dccd-8ee1c821ee64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19220\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "dataset=[]\n",
    "i=0\n",
    "for p in df['data']:\n",
    "  feature=find_features(p)\n",
    "  dataset.append([feature,df['labels'][i]])\n",
    "  i+=1\n",
    "random.shuffle(dataset)\n",
    "\n",
    "print(len(dataset))# dataset is a bag of model of vectors\n",
    "\n",
    "# training_set = dataset[:15000]\n",
    "# testing_set = dataset[15000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WZxs_e7DBsrX"
   },
   "source": [
    "Applying Machine Learning Algorithm as trained earlier that is SVM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1opbRFq6B3b"
   },
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "for r in dataset:\n",
    "  predictions.append(SVC_clf.classify(r[0]))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ghezcN_7FeSu"
   },
   "outputs": [],
   "source": [
    "ground_truth=[]\n",
    "for r in dataset:\n",
    "  ground_truth.append(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "RKlHwFDJEN7I",
    "outputId": "cb3dc85c-42eb-4f29-8e09-c31ddbcaafaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector classifier accuracy : 0.8559833506763788\n",
      "Support Vector f1 score : 0.917653358719581\n"
     ]
    }
   ],
   "source": [
    "print(str,'classifier accuracy :',accuracy_score(ground_truth,predictions))\n",
    "print(str,'f1 score :',f1_score(ground_truth,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUxS_PnuCj4D"
   },
   "source": [
    "Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Esr7XVC86UlY"
   },
   "outputs": [],
   "source": [
    "positive=0\n",
    "negative=0\n",
    "for i in range(0,len(predictions)):\n",
    "  if predictions[i]==1:\n",
    "    positive=positive+1\n",
    "  else:\n",
    "    negative=negative+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p985xF4Q6dN9"
   },
   "outputs": [],
   "source": [
    "positive_percent=positive/(positive+negative)\n",
    "negative_percent=negative/(positive+negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MCZDlRGD6hg7",
    "outputId": "829fcbb8-e3d6-4818-ad85-f6a9f43a0a48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.15712799167534 5.842872008324662\n"
     ]
    }
   ],
   "source": [
    "print(positive_percent*100,negative_percent*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNUOX0c7N1Lr"
   },
   "source": [
    "So this politician (politician Y) has also 94% positive reviews and 6 % negative reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdeQdvN86mrg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "MinorProject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
